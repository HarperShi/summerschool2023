{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "9GMERO5u0s2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33e445ac-53a6-4834-bb05-5028b54453b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install numpy==1.23.5 #restart runtime after upgrading to use this version"
      ],
      "metadata": {
        "id": "kA8y2nHdcnwU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7d657d-3347-4282-cbf2-aa8594a86e39"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "The folder you are executing pip from can no longer be found.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pymcd\n",
        "%pip install pesq"
      ],
      "metadata": {
        "id": "FKJxEnB_4rzJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab_Notebooks/summerschool2023"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlkZbPefn4RY",
        "outputId": "b64154e3-32e2-469b-d47d-1085097f9773"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/summerschool2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import librosa\n",
        "import csv\n",
        "from scipy.io import wavfile\n",
        "from scipy.signal import resample\n",
        "from scipy.stats  import pearsonr\n",
        "from pesq import pesq\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "Mlw1dWuTFc6A"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the header rows for result CSV file\n",
        "GT_dir = \"data/raw\"\n",
        "resyn_dir = \"data/resyn\"\n",
        "degraded_dir = \"data/degraded\"\n",
        "data_dirs = [GT_dir, resyn_dir, degraded_dir]\n",
        "csv_dir_0 = \"result/summary_\"\n",
        "result_csv_paths = [csv_dir_0+name for name in [\"resyn.csv\", \"degraded.csv\"]]\n",
        "#result_csv_paths[0] for resynthesized audios\n",
        "#result_csv_paths[1] for degraded audios\n",
        "\n",
        "\n",
        "# Initialize csvfile, only be run once!\n",
        "for (data_dir, result_csv_path) in zip(data_dirs,result_csv_paths) :\n",
        "  audio_files = [f for f in os.listdir(data_dir) if f.endswith('.wav')]\n",
        "  audio_file_names = [os.path.splitext(f)[0] for f in audio_files]\n",
        "  audio_file_names.sort()\n",
        "  init = {'audio_file_name':audio_file_names}\n",
        "\n",
        "  # Initialize a new csv file with dataframe\n",
        "  df = pd.DataFrame(init)\n",
        "  df.to_csv(result_csv_path)"
      ],
      "metadata": {
        "id": "1w5b1X_3Mqsx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic"
      ],
      "metadata": {
        "id": "mMpgR2VI_AGT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mel cepstral distortion (MCD)\n"
      ],
      "metadata": {
        "id": "N5RBX13w_FRg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mel Cepstral Distortion (MCD) is measure of the difference between two sets of Mel Frequency Cepstral Coefficients(MFCCs). MFCCs are used to represent the spectral envolope of an audio signal and are calculated from Mel-spectrum. \n",
        "\n",
        "MCD is a popular objective evluation metric of the similarity between two audios. A lower MCD indicates greater similarity. And a higher MCD indictes greater differences between them[2].\n",
        "\n",
        "The pymcd package provides scripts to compute a variety of forms of MCD score:\n",
        "\n",
        "* MCD (plain): the conventional MCD metric, which requires the lengths of two input speeches to be the same. Otherwise, it would simply extend the shorted speech to the length of longer one by padding zero for the time-domain waveform.\n",
        "* MCD-DTW: an improved MCD metric that adopts the Dynamic Time Warping (DTW) algorithm to find the minimum MCD between two speeches.\n",
        "* MCD-DTW-SL: MCD-DTW weighted by Speech Length (SL) evaluates both the length and the quality of alignment between two speeches. Based on the MCD-DTW metric, the MCD-DTW-SL incorporates an additional coefficient w.r.t. the difference between the lengths of two speeches[3]."
      ],
      "metadata": {
        "id": "Zlkj7a2I_MW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pymcd.mcd import Calculate_MCD"
      ],
      "metadata": {
        "id": "9Fd8PJkRD0u1"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate one pair\n",
        "\n",
        "# three different modes \"plain\", \"dtw\" and \"dtw_sl\" for the above three MCD metrics \n",
        "MCD_mode = \"dtw\"\n",
        "mcd_toolbox = Calculate_MCD(MCD_mode=MCD_mode)\n",
        "\n",
        "# two inputs w.r.t. reference (ground-truth) and degraded/resynthesized speeches, respectively\n",
        "audio_ref_path = \"data/raw/neutral_sent001_long.wav\"\n",
        "audio_degraded_path = \"data/degraded/neutral_sent001_long.wav\"\n",
        "mcd_value = mcd_toolbox.calculate_mcd(audio_ref_path, audio_degraded_path)\n",
        "\n",
        "print(f\"mcd({MCD_mode}) = {mcd_value:.3f}\")"
      ],
      "metadata": {
        "id": "mieHMHUcESkY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80be60d7-4c8d-4d1f-b1d1-9668269ca190"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mcd(dtw) = 4.137\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate all pairs\n",
        "\n",
        "# three different modes \"plain\", \"dtw\" and \"dtw_sl\" for the above three MCD metrics \n",
        "MCD_mode = \"dtw\"\n",
        "mcd_toolbox = Calculate_MCD(MCD_mode=MCD_mode)\n",
        "\n",
        "# directories of audios from different audio folders\n",
        "GT_dir = \"data/raw\"\n",
        "resyn_dir = \"data/resyn\"\n",
        "degraded_dir = \"data/degraded\"\n",
        "\n",
        "# Evaluate all audios in the folders\n",
        "audio_names = []\n",
        "for subdir, dirs, files in os.walk(GT_dir):\n",
        "  for file in files:\n",
        "      if (\".wav\" in file):\n",
        "          audio_names.append(file)\n",
        "\n",
        "audio_names.sort()\n",
        "mcd_values_resyn = []\n",
        "mcd_values_degraded = []\n",
        "for i in tqdm(range(len(audio_names))):\n",
        "  audio_name = audio_names[i]\n",
        "  GT_audio_path = os.path.join(GT_dir,audio_name)\n",
        "  resyn_audio_path = os.path.join(resyn_dir,audio_name)\n",
        "  degraded_audio_path = os.path.join(degraded_dir,audio_name)\n",
        "  mcd_values_resyn.append(mcd_toolbox.calculate_mcd(GT_audio_path, resyn_audio_path))\n",
        "  mcd_values_degraded.append(mcd_toolbox.calculate_mcd(GT_audio_path, degraded_audio_path))\n",
        "\n",
        "#Write result in csv file\n",
        "df = pd.read_csv(result_csv_paths[0])\n",
        "df['MCD'] = mcd_values_resyn\n",
        "df.to_csv(result_csv_paths[0],index=False)\n",
        "\n",
        "df = pd.read_csv(result_csv_paths[1])\n",
        "df['MCD'] = mcd_values_degraded\n",
        "df.to_csv(result_csv_paths[1],index=False)\n",
        "\n",
        "# Calculate the mean score of MCD\n",
        "mcd_resyn = np.mean(mcd_values_resyn)\n",
        "mcd_degraded = np.mean(mcd_values_degraded)\n",
        "\n",
        "print(\"\\n -------------------------------------\")\n",
        "print(f\"mcd({MCD_mode}) of resynthesized audios = {mcd_resyn:.3f}\")\n",
        "print(f\"mcd({MCD_mode}) of degraded audios = {mcd_degraded:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQnhfAGaGOck",
        "outputId": "a841e2d1-a3a6-4928-925b-40b2bb76919e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:34<00:00,  3.45s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " -------------------------------------\n",
            "mcd(dtw) of resynthesized audios = 2.969\n",
            "mcd(dtw) of degraded audios = 3.618\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F0-PCC"
      ],
      "metadata": {
        "id": "NxB61M2x0H_F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$F_{0}-PCC$ is the Pearson Correlation Coefficient (PCC) between F0 vecotrs from two audios. The fundamental frequency(F0) of a sound wave represents the perceived pitch of the sound, and can be used to distinguish different speech sounds. \n",
        "\n",
        "Pearson correlation coefficient (PCC) is a measure of the linear correlation between two variables. It is commonly used in statistics to measure the strength and direction of the relationship between two sets of data. It ranges from -1 to 1, where -1 indicates a perfect negative correlation, 0 indicates no correlation and 1 indicates perfect positive correlation[9]. "
      ],
      "metadata": {
        "id": "iQMLFmGo0L4F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_F0PCC(audio_ref_path, audio_synthesized_path):\n",
        "  audio_ref, rate= librosa.load(audio_ref_path)\n",
        "  audio_synthesized, rate= librosa.load(audio_synthesized_path)\n",
        "\n",
        "  #audios have different lenghts\n",
        "  if len(audio_ref)-len(audio_synthesized)>=0:\n",
        "    audio_synthesized = np.pad(audio_synthesized, (0, len(audio_ref)-len(audio_synthesized)), 'constant', constant_values=(0, 0)) \n",
        "  else:\n",
        "    audio_synthesized = audio_synthesized[:len(audio_ref)]\n",
        "\n",
        "  f0_ref, voiced_flag_ref, voiced_probs_ref = librosa.pyin(audio_ref, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "  f0_synthesized, voiced_flag_synthesized, voiced_probs_synthesized = librosa.pyin(audio_synthesized, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "  f0_ref = np.nan_to_num((f0_ref))\n",
        "  f0_synthesized = np.nan_to_num(f0_synthesized)\n",
        "  \n",
        "  f0_pcc = pearsonr(f0_ref, f0_synthesized)[0]\n",
        "\n",
        "  return f0_pcc"
      ],
      "metadata": {
        "id": "XuNNFxop3FMH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate one pair\n",
        "\n",
        "# two inputs w.r.t. reference (ground-truth) and degraded/resynthesized speeches, respectively\n",
        "audio_ref_path = \"data/raw/neutral_sent001_short.wav\"\n",
        "audio_degraded_path = \"data/resyn/neutral_sent001_short.wav\"\n",
        "\n",
        "f0_pcc = calculate_F0PCC(audio_ref_path, audio_degraded_path)\n",
        "\n",
        "print(f\"F0-PCC = {f0_pcc:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGi6moaa4r4z",
        "outputId": "46fddef5-b6a4-438d-97b6-0955a6eb793d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F0-PCC = 0.703\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate all pairs\n",
        "\n",
        "# directories of audios from different audio folders\n",
        "GT_dir = \"data/raw\"\n",
        "resyn_dir = \"data/resyn\"\n",
        "degraded_dir = \"data/degraded\"\n",
        "\n",
        "# Evaluate all audios in the folders\n",
        "audio_names = []\n",
        "for subdir, dirs, files in os.walk(GT_dir):\n",
        "  for file in files:\n",
        "      if (\".wav\" in file):\n",
        "          audio_names.append(file)\n",
        "\n",
        "audio_names.sort()\n",
        "f0pcc_values_resyn = []\n",
        "f0pcc_values_degraded = []\n",
        "for i in tqdm(range(len(audio_names))):\n",
        "  audio_name = audio_names[i]\n",
        "  GT_audio_path = os.path.join(GT_dir,audio_name)\n",
        "  resyn_audio_path = os.path.join(resyn_dir,audio_name)\n",
        "  degraded_audio_path = os.path.join(degraded_dir,audio_name)\n",
        "  f0pcc_values_resyn.append(calculate_F0PCC(GT_audio_path, resyn_audio_path))\n",
        "  f0pcc_values_degraded.append(calculate_F0PCC(GT_audio_path, degraded_audio_path))\n",
        "\n",
        "#Write result in csv file\n",
        "df = pd.read_csv(result_csv_paths[0])\n",
        "df['F0_PCC'] = f0pcc_values_resyn\n",
        "df.to_csv(result_csv_paths[0],index=False)\n",
        "\n",
        "df = pd.read_csv(result_csv_paths[1])\n",
        "df['F0_PCC'] = f0pcc_values_degraded\n",
        "df.to_csv(result_csv_paths[1],index=False)\n",
        "\n",
        "# Calculate the mean score of F0_PCC\n",
        "f0pcc_resyn = np.mean(f0pcc_values_resyn)\n",
        "f0pcc_degraded = np.mean(f0pcc_values_degraded)\n",
        "\n",
        "print(\"\\n -------------------------------------\")\n",
        "print(f\"F0_PCC of resynthesized audios = {f0pcc_resyn:.3f}\")\n",
        "print(f\"F0 PCC of degraded audios = {f0pcc_degraded:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S-2uVvD7lqA",
        "outputId": "b532ae34-476e-455a-cdd4-454e8e8c76a5"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 3/10 [00:44<01:40, 14.37s/it]/usr/local/lib/python3.10/dist-packages/scipy/stats/_stats_py.py:4424: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
            "  warnings.warn(stats.ConstantInputWarning(msg))\n",
            "100%|██████████| 10/10 [01:56<00:00, 11.63s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " -------------------------------------\n",
            "F0_PCC of resynthesized audios = nan\n",
            "F0 PCC of degraded audios = nan\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perceptual Evaluation of Speech Quality (PESQ)"
      ],
      "metadata": {
        "id": "-6QT0KLAQb6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PESQ is an objective measure of audio quality that compares an audio output to the original voice file, taking into account factors such as audio sharpness, call volume, background noise, latency, clipping, and audio interference. PESQ returns a score between -0.5 and 4.5, with higher scores indicating better quality.\n",
        "\n",
        "To evaluate audios using PESQ, the original and degraded signals are level equalized, filtered, time-aligned, and processed through an auditory transform to obtain the loudness spectra. The difference in loudness between the original and degraded signals is then computed and averaged over time and frequency to produce a prediction of subjective quality rating[8]."
      ],
      "metadata": {
        "id": "ci7xUGA2w5Ff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_PESQ(audio_ref_path, audio_synthesized_path):\n",
        "  fs,audio_ref = wavfile.read(audio_ref_path)\n",
        "  fs,audio_synthesized =  wavfile.read(audio_synthesized_path)\n",
        "\n",
        "  #audios have different lenghts\n",
        "  if len(audio_ref)-len(audio_synthesized)>=0:\n",
        "    audio_synthesized = np.pad(audio_synthesized, (0, len(audio_ref)-len(audio_synthesized)), 'constant', constant_values=(0, 0)) \n",
        "  else:\n",
        "    audio_synthesized = audio_synthesized[:len(audio_ref)]\n",
        "\n",
        "  num_samples_ref = round(len(audio_ref)/fs*16000) #downsample to 16kH\n",
        "  num_samples_synthesized = round(len(audio_synthesized)/fs*16000) #downsample to 16kH\n",
        "  ref_speech_16k = resample(audio_synthesized, num_samples_ref)\n",
        "  synthesized_speech_16k = resample(audio_synthesized, num_samples_synthesized)\n",
        "\n",
        "  PESQ = pesq(16000, audio_ref, audio_synthesized)\n",
        "\n",
        "  return PESQ"
      ],
      "metadata": {
        "id": "fIvUsT5fQ_tg"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate single audio\n",
        "\n",
        "# two inputs w.r.t. reference (ground-truth) and degraded/resynthesized speeches, respectively\n",
        "audio_ref_path = \"data/raw/neutral_sent001_long.wav\"\n",
        "audio_degraded_path = \"data/degraded/neutral_sent001_long.wav\"\n",
        "\n",
        "PESQ = calculate_PESQ(audio_ref_path, audio_degraded_path)\n",
        "\n",
        "print(f\"PESQ = {PESQ:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Co_DTGEiSAbW",
        "outputId": "4bf2a9d5-2f88-4d4f-e354-d59ee66658c3"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PESQ = 1.136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate all pairs\n",
        "\n",
        "# directories of audios from different models\n",
        "GT_dir = \"data/raw\"\n",
        "resyn_dir = \"data/resyn\"\n",
        "degraded_dir = \"data/degraded\"\n",
        "\n",
        "# Evaluate all audios in the folders\n",
        "audio_names = []\n",
        "for subdir, dirs, files in os.walk(GT_dir):\n",
        "  for file in files:\n",
        "      if (\".wav\" in file):\n",
        "          audio_names.append(file)\n",
        "\n",
        "audio_names.sort()\n",
        "PESQ_values_resyn = []\n",
        "PESQ_values_degraded = []\n",
        "for i in tqdm(range(len(audio_names))):\n",
        "  audio_name = audio_names[i]\n",
        "  GT_audio_path = os.path.join(GT_dir,audio_name)\n",
        "  resyn_audio_path = os.path.join(resyn_dir,audio_name)\n",
        "  degraded_audio_path = os.path.join(degraded_dir,audio_name)\n",
        "  PESQ_values_resyn.append(calculate_PESQ(GT_audio_path, resyn_audio_path))\n",
        "  PESQ_values_degraded.append(calculate_PESQ(GT_audio_path, degraded_audio_path))\n",
        "\n",
        "#Write result in csv file\n",
        "df = pd.read_csv(result_csv_paths[0])\n",
        "df['PESQ'] = PESQ_values_resyn\n",
        "df.to_csv(result_csv_paths[0],index=False)\n",
        "\n",
        "df = pd.read_csv(result_csv_paths[1])\n",
        "df['PESQ'] = PESQ_values_degraded\n",
        "df.to_csv(result_csv_paths[1],index=False)\n",
        "\n",
        "# Calculate the mean score of PESQ\n",
        "PESQ_resyn = np.mean(PESQ_values_resyn)\n",
        "PESQ_degraded = np.mean(PESQ_values_degraded)\n",
        "\n",
        "print(\"\\n -------------------------------------\")\n",
        "print(f\"PESQ of resyn = {PESQ_resyn:.3f}\")\n",
        "print(f\"PESQ of degraded = {PESQ_degraded:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sm0wI_5HSjMu",
        "outputId": "acfda7ba-7f98-460b-b7f5-6fefbe649eb1"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:05<00:00,  1.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " -------------------------------------\n",
            "PESQ of resyn = 2.491\n",
            "PESQ of degraded = 1.233\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced"
      ],
      "metadata": {
        "id": "TEHqNAtdQDWn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MOSNet"
      ],
      "metadata": {
        "id": "HAOajRGlBC-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "MOSNet is a deep learning model that uses convolutional and recurrent neural network models to predict Mean Opinion Scores (MOS) for audio signals. The model was tested on a large-scale listening test results of the Voice Conversion Challenge (VCC) 2018,  it learns to predict the MOS score of an audio signal based on its features. The features used by MOSNet include time-domain and frequency-domain representations of the audio signal, as well as higher-level features such as loudness, sharpness, and roughness[5]."
      ],
      "metadata": {
        "id": "5VuT_vK52PM7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd MOSNet\n",
        "\n",
        "%run custom_test.py --rootdir \"../data/resyn\"\n",
        "df = pd.read_csv(\"../\"+result_csv_paths[0])\n",
        "df['MOSNet'] = MOSNet_results\n",
        "df.to_csv(\"../\"+result_csv_paths[0],index=False)\n",
        "\n",
        "%run custom_test.py --rootdir \"../data/degraded\"\n",
        "df = pd.read_csv(\"../\"+result_csv_paths[1])\n",
        "df['MOSNet'] = MOSNet_results\n",
        "df.to_csv(\"../\"+result_csv_paths[1],index=False)\n",
        "\n",
        "## go back to the directory of summerschool2023\n",
        "%cd - "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gh6W9XOlBGEY",
        "outputId": "650ba8b6-64bd-4b1c-8eca-768499386fa4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/summerschool2023/MOSNet\n",
            "Loading model weights\n",
            "CNN_BLSTM init\n",
            "Start evaluating 10 waveforms...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:06<00:00,  1.66it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average: 3.0965\n",
            "Loading model weights\n",
            "CNN_BLSTM init\n",
            "Start evaluating 10 waveforms...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:12<00:00,  1.29s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average: 3.3382000000000005\n",
            "/content/drive/MyDrive/Colab_Notebooks/summerschool2023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ViSQOL (Virtual Speech Quality Objective Listener) "
      ],
      "metadata": {
        "id": "Xmldxa5PQeMa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ViSQOL (Virtual Speech Quality Objective Listener) is an objective, full-reference metric for perceived audio quality. It uses a spectro-temporal measure of similarity between a reference and a test speech signal to produce a MOS-LQO (Mean Opinion Score - Listening Quality Objective) score. MOS-LQO scores range from 1 (the worst) to 5 (the best)[10]."
      ],
      "metadata": {
        "id": "2OkB41GHQlgF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Since there is no wrapped python library, we have to install the python api from the first step.\n",
        "# requires a very long running time ~45min(build with bazel) + 36min(import as python package)"
      ],
      "metadata": {
        "id": "eVmFbA4dPRNV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/google/visqol.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXS2lGRWgH-L",
        "outputId": "3fe4c380-a834-46c1-f853-0ecf1623c911"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory\n",
            "fatal: could not create work tree dir 'visqol': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd visqol"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NcvRj4SA9HT_",
        "outputId": "9a1b3ed3-f66a-4471-dd4c-fc71a1628023"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'visqol'\n",
            "/content/drive/MyDrive/Colab_Notebooks/summerschool2023/visqol\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g @bazel/bazelisk ##install bazel "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9X0I2kL6Ue3",
        "outputId": "bb86456c-c7d0-456c-9a15-dc2a5368a4a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/bazelisk -> /tools/node/lib/node_modules/@bazel/bazelisk/bazelisk.js\n",
            "/tools/node/bin/bazel -> /tools/node/lib/node_modules/@bazel/bazelisk/bazelisk.js\n",
            "+ @bazel/bazelisk@0.0.0-PLACEHOLDER\n",
            "updated 1 package in 0.575s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!touch WORKSPACE"
      ],
      "metadata": {
        "id": "xFVTiETG8gnL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!bazel build :visqol -c opt  ### take about 45min"
      ],
      "metadata": {
        "id": "WMAR2twU8K1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install . #install visqol in python, takes about 36min"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OiydKtnKLW64",
        "outputId": "ad7060a0-9532-42af-d2a1-b8336423453c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Processing /content/drive/MyDrive/Colab_Notebooks/summerschool2023/visqol\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "from visqol import visqol_lib_py\n",
        "from visqol.pb2 import visqol_config_pb2\n",
        "from visqol.pb2 import similarity_result_pb2\n",
        "\n",
        "config = visqol_config_pb2.VisqolConfig()\n",
        "\n",
        "mode = \"speech\"\n",
        "if mode == \"audio\":\n",
        "    config.audio.sample_rate = 48000\n",
        "    config.options.use_speech_scoring = False\n",
        "    svr_model_path = \"libsvm_nu_svr_model.txt\"\n",
        "elif mode == \"speech\":\n",
        "    config.audio.sample_rate = 16000\n",
        "    config.options.use_speech_scoring = True\n",
        "    svr_model_path = \"lattice_tcditugenmeetpackhref_ls2_nl60_lr12_bs2048_learn.005_ep2400_train1_7_raw.tflite\"\n",
        "else:\n",
        "    raise ValueError(f\"Unrecognized mode: {mode}\")\n",
        "\n",
        "config.options.svr_model_path = os.path.join(\n",
        "    os.path.dirname(visqol_lib_py.__file__), \"model\", svr_model_path)\n",
        "\n",
        "api = visqol_lib_py.VisqolApi()\n",
        "\n",
        "api.Create(config)"
      ],
      "metadata": {
        "id": "3DgzeRdqLaML",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "3b606611-3e09-4137-90d8-44ef6fd80ca3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-0c5005fa0f76>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mvisqol\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisqol_lib_py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvisqol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mvisqol_config_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mvisqol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpb2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msimilarity_result_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'visqol'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate one pair\n",
        "\n",
        "audio_ref_path = \"data/raw/neutral_sent001_long.wav\"\n",
        "audio_degraded_path = \"data/degraded/neutral_sent001_long.wav\"\n",
        "\n",
        "rate_ref, audio_ref= wavfile.read(audio_ref_path)\n",
        "rate_degraded, audio_degraded= wavfile.read(audio_degraded_path)\n",
        "#!!! audios need to be changed to float64\n",
        "\n",
        "similarity_result = api.Measure(audio_ref.astype(np.float64), audio_degraded.astype(np.float64))\n",
        "\n",
        "print(similarity_result.moslqo)"
      ],
      "metadata": {
        "id": "zHoSPx_ta4pc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93377c4b-b396-44fc-bb25-cb26ecf90a58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.9846615563441414\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd - \n",
        "## make sure you are under the folder summerschool2023"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RCARI8RZlrxF",
        "outputId": "ac2b2d85-44e0-4be7-a29b-e430be147ce3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/summerschool2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate all pairs\n",
        "\n",
        "# directories of audios from different models\n",
        "GT_dir = \"data/ground_truth\"\n",
        "resyn_dir = \"data/resyn\"\n",
        "degraded_dir = \"data/degraded\"\n",
        "\n",
        "# Evaluate all audios in the folders\n",
        "audio_names = []\n",
        "for subdir, dirs, files in os.walk(GT_dir):\n",
        "  for file in files:\n",
        "      if (\".wav\" in file):\n",
        "          audio_names.append(file)\n",
        "\n",
        "audio_names.sort()\n",
        "visqol_values_resyn = []\n",
        "visqol_values_degraded = []\n",
        "for i in tqdm(range(len(audio_names))):\n",
        "  audio_name = audio_names[i]\n",
        "  GT_audio_path = os.path.join(GT_dir,audio_name)\n",
        "  resyn_audio_path = os.path.join(resyn_dir,audio_name)\n",
        "  degraded_audio_path = os.path.join(degraded_dir,audio_name)\n",
        "\n",
        "  _, GT_audio= wavfile.read(GT_audio_path)\n",
        "  _, resyn_audio= wavfile.read(resyn_audio_path)\n",
        "  _, degraded_audio= wavfile.read(degraded_audio_path)\n",
        "\n",
        "  visqol_values_resyn.append(api.Measure(GT_audio.astype(np.float64), resyn_audio.astype(np.float64)).moslqo)\n",
        "  visqol_values_degraded.append(api.Measure(GT_audio.astype(np.float64), degraded_audio.astype(np.float64)).moslqo)\n",
        "\n",
        "#Write result in csv file\n",
        "df = pd.read_csv(result_csv_paths[0])\n",
        "df['ViSQOL'] = visqol_values_resyn\n",
        "df.to_csv(result_csv_paths[0],index=False)\n",
        "\n",
        "df = pd.read_csv(result_csv_paths[1])\n",
        "df['ViSQOL'] = visqol_values_degraded\n",
        "df.to_csv(result_csv_paths[1],index=False)\n",
        "\n",
        "# Calculate the mean score of ViSQOL\n",
        "visqol_resyn = np.mean(visqol_values_resyn)\n",
        "visqol_degraded = np.mean(visqol_values_degraded)\n",
        "\n",
        "print(\"\\n -------------------------------------\")\n",
        "print(f\"ViSQOL of resyn = {visqol_resyn:.3f}\")\n",
        "print(f\"ViSQOL of degraded = {visqol_degraded:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1kXd2ooUjRMu",
        "outputId": "e3cbf5f8-e76b-4faf-b20e-62ed673c69ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:35<00:00,  7.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " -------------------------------------\n",
            "ViSQOL of Tacotron2 = 2.843\n",
            "ViSQOL of FastPitch = 2.961\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Others"
      ],
      "metadata": {
        "id": "sY4lmkbIhftb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Voicing Decision Error (VDE)"
      ],
      "metadata": {
        "id": "CZpgWM5mP8ZD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Voicing Decision Error (VDE) is a measure of the accuracy of voicing detection in speech processing. It measures the error in determining whether a speech segment is voiced or unvoiced and is commonly used in speech recognition and speaker verification applications. VDE can be calculated by comparing the voicing decisions made by the algorithm to the ground truth voicing information and is usually expressed as a percentage. Lower VDE values indicate better performance of the voicing detection algorithm[4].\n",
        "\n",
        "$ VDE = \\frac{N_{V\\rightarrow U}+N_{U\\rightarrow V}}{N}× 100\\%$"
      ],
      "metadata": {
        "id": "NtTv8BownzZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_vde(audio_ref_path, audio_synthesized_path):\n",
        "  audio_ref, rate= librosa.load(audio_ref_path)\n",
        "  audio_synthesized, rate= librosa.load(audio_synthesized_path)\n",
        "\n",
        "  #audios have different lenghts\n",
        "  if len(audio_ref)-len(audio_synthesized)>=0:\n",
        "    audio_synthesized = np.pad(audio_synthesized, (0, len(audio_ref)-len(audio_synthesized)), 'constant', constant_values=(0, 0)) \n",
        "  else:\n",
        "    audio_synthesized = audio_synthesized[:len(audio_ref)]\n",
        "\n",
        "  f0_ref, voiced_flag_ref, voiced_probs_ref = librosa.pyin(audio_ref, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "  f0_synthesized, voiced_flag_synthesized, voiced_probs_synthesized = librosa.pyin(audio_synthesized, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "\n",
        "  num_flag_error = 0\n",
        "  num_flag_total = len(voiced_flag_ref)\n",
        "  for i in range(num_flag_total):\n",
        "    if voiced_flag_ref[i] != voiced_flag_synthesized[i]:\n",
        "      num_flag_error+=1\n",
        "      \n",
        "  vde = num_flag_error/num_flag_total\n",
        "\n",
        "  return vde"
      ],
      "metadata": {
        "id": "8iku1s4qtYr0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate one pair\n",
        "\n",
        "# two inputs w.r.t. reference (ground-truth) and degraded/resynthesized speeches, respectively\n",
        "audio_ref_path = \"data/raw/neutral_sent001_long.wav\"\n",
        "audio_degraded_path = \"data/degraded/neutral_sent001_long.wav\"\n",
        "\n",
        "vde = calculate_vde(audio_ref_path, audio_degraded_path)\n",
        "\n",
        "print(f\"VDE = {vde:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Z-6HMI3g8HF",
        "outputId": "e3029561-8865-4636-ee09-11265147fa00"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VDE = 0.219\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate all pairs\n",
        "\n",
        "# directories of audios from different models\n",
        "GT_dir = \"data/raw\"\n",
        "resyn_dir = \"data/resyn\"\n",
        "degraded_dir = \"data/degraded\"\n",
        "\n",
        "# Evaluate all audios in the folders\n",
        "audio_names = []\n",
        "for subdir, dirs, files in os.walk(GT_dir):\n",
        "  for file in files:\n",
        "      if (\".wav\" in file):\n",
        "          audio_names.append(file)\n",
        "\n",
        "audio_names.sort()\n",
        "vde_values_resyn = []\n",
        "vde_values_degraded = []\n",
        "for i in tqdm(range(len(audio_names))):\n",
        "  audio_name = audio_names[i]\n",
        "  GT_audio_path = os.path.join(GT_dir,audio_name)\n",
        "  resyn_audio_path = os.path.join(resyn_dir,audio_name)\n",
        "  degraded_audio_path = os.path.join(degraded_dir,audio_name)\n",
        "  vde_values_resyn.append(calculate_vde(GT_audio_path, resyn_audio_path))\n",
        "  vde_values_degraded.append(calculate_vde(GT_audio_path, degraded_audio_path))\n",
        "\n",
        "#Write result in csv file\n",
        "df = pd.read_csv(result_csv_paths[0])\n",
        "df['VDE'] = vde_values_resyn\n",
        "df.to_csv(result_csv_paths[0],index=False)\n",
        "\n",
        "df = pd.read_csv(result_csv_paths[1])\n",
        "df['VDE'] = vde_values_degraded\n",
        "df.to_csv(result_csv_paths[1],index=False)\n",
        "\n",
        "# Calculate the mean score of VDE\n",
        "vde_resyn = np.mean(vde_values_resyn)\n",
        "vde_degraded = np.mean(vde_values_degraded)\n",
        "\n",
        "print(\"\\n -------------------------------------\")\n",
        "print(f\"VDE of resyn = {vde_resyn:.3f}\")\n",
        "print(f\"VDE of degraded = {vde_degraded:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e83UCe_A4t7c",
        "outputId": "37a00abd-a2e9-4855-8cfc-d631371a5805"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:52<00:00, 11.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " -------------------------------------\n",
            "VDE of resyn = 0.097\n",
            "VDE of degraded = 0.136\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gross Pitch Error (GPE) "
      ],
      "metadata": {
        "id": "fHc6SY-kOnxs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Gross Pitch Error (GPE) is a measure of the accuracy of pitch estimation in speech processing. It is defined as the percentage of frames or segments in which the estimated fundamental frequency deviates from the true fundamental frequency by more than a predefined threshold. A high GPE score indicates that the pitch estimation algorithm is making many errors in estimating the fundamental frequency of the speech signal. GPE is commonly used to evaluate the performance of speech analysis and synthesis systems[4].\n",
        "\n",
        "\n",
        "$GPE=\\frac{N_{F 0 E}}{N_{V V}} \\times 100 \\%$\n",
        "\n",
        "$\\left|\\frac{F 0_{i, \\text { estimated }}}{F 0_{i, \\text { reference }}}-1\\right|>\\delta \\%$"
      ],
      "metadata": {
        "id": "puaN3nZuqmLK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_gpe(audio_ref_path, audio_synthesized_path):\n",
        "  audio_ref, rate= librosa.load(audio_ref_path)\n",
        "  audio_synthesized, rate= librosa.load(audio_synthesized_path)\n",
        "\n",
        "  #audios have different lenghts\n",
        "  if len(audio_ref)-len(audio_synthesized)>=0:\n",
        "    audio_synthesized = np.pad(audio_synthesized, (0, len(audio_ref)-len(audio_synthesized)), 'constant', constant_values=(0, 0)) \n",
        "  else:\n",
        "    audio_synthesized = audio_synthesized[:len(audio_ref)]\n",
        "\n",
        "  f0_ref, voiced_flag_ref, voiced_probs_ref = librosa.pyin(audio_ref, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "  f0_synthesized, voiced_flag_synthesized, voiced_probs_synthesized = librosa.pyin(audio_synthesized, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "\n",
        "  num_f0_error = 0\n",
        "  num_f0_total = 0\n",
        "  for i in range(len(f0_ref)):\n",
        "    if voiced_flag_ref[i]==True and  voiced_flag_synthesized[i]==True:\n",
        "      num_f0_total+=1\n",
        "      if np.abs(f0_synthesized[i]/f0_ref[i]-1)>0.2:\n",
        "        num_f0_error+=1\n",
        "\n",
        "  gpe = num_f0_error/num_f0_total\n",
        "\n",
        "  return gpe"
      ],
      "metadata": {
        "id": "jbTrE9gHszTW"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate one pair\n",
        "\n",
        "# two inputs w.r.t. reference (ground-truth) and degraded/resynthesized speeches, respectively\n",
        "audio_ref_path = \"data/raw/neutral_sent001_long.wav\"\n",
        "audio_degraded_path = \"data/degraded/neutral_sent001_long.wav\"\n",
        "\n",
        "gpe = calculate_gpe(audio_ref_path, audio_degraded_path)\n",
        "\n",
        "print(f\"GPE = {gpe:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-mKBB5_bnzd1",
        "outputId": "b89bbc25-a038-4fd0-ef27-13cd896199ae"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPE = 0.421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate all pairs\n",
        "\n",
        "# directories of audios from different models\n",
        "GT_dir = \"data/raw\"\n",
        "resyn_dir = \"data/resyn\"\n",
        "degraded_dir = \"data/degraded\"\n",
        "\n",
        "# Evaluate all audios in the folders\n",
        "audio_names = []\n",
        "for subdir, dirs, files in os.walk(GT_dir):\n",
        "  for file in files:\n",
        "      if (\".wav\" in file):\n",
        "          audio_names.append(file)\n",
        "\n",
        "audio_names.sort()\n",
        "gpe_values_resyn = []\n",
        "gpe_values_degraded = []\n",
        "for i in tqdm(range(len(audio_names))):\n",
        "  audio_name = audio_names[i]\n",
        "  GT_audio_path = os.path.join(GT_dir,audio_name)\n",
        "  resyn_audio_path = os.path.join(resyn_dir,audio_name)\n",
        "  degraded_audio_path = os.path.join(degraded_dir,audio_name)\n",
        "  gpe_values_resyn.append(calculate_gpe(GT_audio_path, resyn_audio_path))\n",
        "  gpe_values_degraded.append(calculate_gpe(GT_audio_path, degraded_audio_path))\n",
        "\n",
        "#Write result in csv file\n",
        "df = pd.read_csv(result_csv_paths[0])\n",
        "df['GPE'] = gpe_values_resyn\n",
        "df.to_csv(result_csv_paths[0],index=False)\n",
        "\n",
        "df = pd.read_csv(result_csv_paths[1])\n",
        "df['GPE'] = gpe_values_degraded\n",
        "df.to_csv(result_csv_paths[1],index=False)\n",
        "\n",
        "# Calculate the mean score of gpe\n",
        "gpe_resyn = np.mean(gpe_values_resyn)\n",
        "gpe_degraded = np.mean(gpe_values_degraded)\n",
        "\n",
        "print(\"\\n -------------------------------------\")\n",
        "print(f\"GPE of resyn = {gpe_resyn:.3f}\")\n",
        "print(f\"GPE of degraded = {gpe_degraded:.3f}\")"
      ],
      "metadata": {
        "id": "9NmFL0dg7Npz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## F0 Frame Error (FFE)"
      ],
      "metadata": {
        "id": "ZQ8jtqISP_dN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "F0 Frame Error (FFE) is a measure of the accuracy of fundamental frequency (F0) estimation in speech processing. It measures the error in estimating the F0 value over a frame of the speech signal. FFE is calculated by comparing the estimated F0 value to the true F0 value for each frame of the speech signal. The error is typically expressed as a percentage of the true F0 value. A high FFE score indicates that the F0 estimation algorithm is making many errors in estimating the F0 value for each frame of the speech signal[4].\n",
        "\n",
        "$\\begin{aligned} FFE & =\\frac{\\# \\text { of error frames }}{\\# \\text { of total frames }} \\times 100 \\% \\\\ = & \\frac{N_{U \\rightarrow V}+N_{V \\rightarrow U}+N_{F 0 E}}{N} \\times 100 \\% .\\end{aligned}$"
      ],
      "metadata": {
        "id": "ie4CMuA7pjLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_ffe(audio_ref_path, audio_synthesized_path):\n",
        "  audio_ref, rate= librosa.load(audio_ref_path)\n",
        "  audio_synthesized, rate= librosa.load(audio_synthesized_path)\n",
        "\n",
        "  #audios have different lenghts\n",
        "  if len(audio_ref)-len(audio_synthesized)>=0:\n",
        "    audio_synthesized = np.pad(audio_synthesized, (0, len(audio_ref)-len(audio_synthesized)), 'constant', constant_values=(0, 0)) \n",
        "  else:\n",
        "    audio_synthesized = audio_synthesized[:len(audio_ref)]\n",
        "\n",
        "  f0_ref, voiced_flag_ref, voiced_probs_ref = librosa.pyin(audio_ref, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "  f0_synthesized, voiced_flag_synthesized, voiced_probs_synthesized = librosa.pyin(audio_synthesized, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
        "\n",
        "  num_flag_error = 0\n",
        "  num_flag_total = len(voiced_flag_ref)\n",
        "  num_f0_error = 0\n",
        "  num_f0_total = 0\n",
        "  for i in range(num_flag_total):\n",
        "    if voiced_flag_ref[i] != voiced_flag_synthesized[i]:\n",
        "      num_flag_error+=1\n",
        "    elif voiced_flag_ref[i]==True and np.abs(f0_synthesized[i]/f0_ref[i]-1)>0.2:\n",
        "      num_f0_error+=1\n",
        "\n",
        "  ffe = (num_flag_error+num_f0_error)/num_flag_total\n",
        "\n",
        "  return ffe"
      ],
      "metadata": {
        "id": "wlpcCunkt6RF"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate one pair\n",
        "\n",
        "# two inputs w.r.t. reference (ground-truth) and degraded/resynthesized speeches, respectively\n",
        "audio_ref_path = \"data/raw/neutral_sent001_long.wav\"\n",
        "audio_degraded_path = \"data/degraded/neutral_sent001_long.wav\"\n",
        "\n",
        "ffe = calculate_ffe(audio_ref_path, audio_degraded_path)\n",
        "\n",
        "print(f\"FFE = {ffe:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YP5hzY8jkvmX",
        "outputId": "421f55d6-0ee7-4067-874d-e3529573683a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FFE = 0.431\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Evaluate all pairs\n",
        "\n",
        "# directories of audios from different models\n",
        "GT_dir = \"data/raw\"\n",
        "resyn_dir = \"data/resyn\"\n",
        "degraded_dir = \"data/degraded\"\n",
        "\n",
        "# Evaluate all audios in the folders\n",
        "audio_names = []\n",
        "for subdir, dirs, files in os.walk(GT_dir):\n",
        "  for file in files:\n",
        "      if (\".wav\" in file):\n",
        "          audio_names.append(file)\n",
        "\n",
        "audio_names.sort()\n",
        "ffe_values_resyn = []\n",
        "ffe_values_degraded = []\n",
        "for i in tqdm(range(len(audio_names))):\n",
        "  audio_name = audio_names[i]\n",
        "  GT_audio_path = os.path.join(GT_dir,audio_name)\n",
        "  resyn_audio_path = os.path.join(resyn_dir,audio_name)\n",
        "  degraded_audio_path = os.path.join(degraded_dir,audio_name)\n",
        "  ffe_values_resyn.append(calculate_ffe(GT_audio_path, resyn_audio_path))\n",
        "  ffe_values_degraded.append(calculate_ffe(GT_audio_path, degraded_audio_path))\n",
        "\n",
        "#Write result in csv file\n",
        "df = pd.read_csv(result_csv_paths[0])\n",
        "df['FFE'] = ffe_values_resyn\n",
        "df.to_csv(result_csv_paths[0],index=False)\n",
        "\n",
        "df = pd.read_csv(result_csv_paths[1])\n",
        "df['FFE'] = ffe_values_degraded\n",
        "df.to_csv(result_csv_paths[1],index=False)\n",
        "\n",
        "# Calculate the mean score of ffe\n",
        "ffe_resyn = np.mean(ffe_values_resyn)\n",
        "ffe_degraded = np.mean(ffe_values_degraded)\n",
        "\n",
        "print(\"\\n -------------------------------------\")\n",
        "print(f\"FFE of resyn = {ffe_resyn:.3f}\")\n",
        "print(f\"FFE of degraded = {ffe_degraded:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NBVcK62GsJKk",
        "outputId": "e181e4ed-3c60-4f9b-dd30-f6932488f41f"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [01:41<00:00, 10.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " -------------------------------------\n",
            "FFE of resyn = 0.105\n",
            "FFE of degraded = 0.393\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NISQA: Speech Quality and Naturalness Assessment"
      ],
      "metadata": {
        "id": "Ba6imyOXeE21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NISQA is a deep learning model/framework for speech quality prediction. The NISQA model weights can be used to predict the quality of a speech sample that has been sent through a communication system (e.g telephone or video call). Besides overall speech quality, NISQA also provides predictions for the quality dimensions Noisiness, Coloration, Discontinuity, and Loudness to give more insight into the cause of the quality degradation[3]."
      ],
      "metadata": {
        "id": "cIhWdJTEsqpu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd NISQA\n",
        "%run run_predict.py --mode predict_file --pretrained_model weights/nisqa_tts.tar --deg \"../data/raw/neutral_sent001_long.wav\" --output_dir \"../result\"\n",
        "## go back to the directory of summerschool2023\n",
        "%cd - "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tsCgSaqeEQg",
        "outputId": "ea545dd8-0aa8-47c8-ffc8-1431e38e0cfc"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/summerschool2023/NISQA\n",
            "Device: cpu\n",
            "Model architecture: NISQA\n",
            "Loaded pretrained model from weights/nisqa_tts.tar\n",
            "---> Predicting ...\n",
            "                     deg  mos_pred        model\n",
            "neutral_sent001_long.wav  2.257622 NISQA_TTS_v1\n",
            "/content/drive/MyDrive/Colab_Notebooks/summerschool2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd NISQA\n",
        "\n",
        "%run run_predict.py --mode predict_dir --pretrained_model weights/nisqa_tts.tar --data_dir \"../data/resyn\" --num_workers 0 --bs 10 \n",
        "df = pd.read_csv(\"../\"+result_csv_paths[0])\n",
        "df['NISQA_MOS'] = nisqa.ds_val.df['mos_pred']\n",
        "df.to_csv(\"../\"+result_csv_paths[0],index=False)\n",
        "\n",
        "%run run_predict.py --mode predict_dir --pretrained_model weights/nisqa_tts.tar --data_dir \"../data/degraded\" --num_workers 0 --bs 10 \n",
        "df = pd.read_csv(\"../\"+result_csv_paths[1])\n",
        "df['NISQA_MOS'] = nisqa.ds_val.df['mos_pred']\n",
        "df.to_csv(\"../\"+result_csv_paths[1],index=False)\n",
        "\n",
        "## go back to the directory of summerschool2023\n",
        "%cd - "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zeuZhtD4hCxP",
        "outputId": "5f9b33a3-c97b-4921-b3ba-9a00a2042a96"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/summerschool2023/NISQA\n",
            "Device: cpu\n",
            "Model architecture: NISQA\n",
            "Loaded pretrained model from weights/nisqa_tts.tar\n",
            "# files: 10\n",
            "---> Predicting ...\n",
            "                      deg  mos_pred\n",
            "neutral_sent005_short.wav  3.864740\n",
            " neutral_sent005_long.wav  2.201691\n",
            "neutral_sent002_short.wav  3.295876\n",
            " neutral_sent002_long.wav  2.845493\n",
            "neutral_sent004_short.wav  2.740637\n",
            " neutral_sent004_long.wav  2.729923\n",
            "neutral_sent003_short.wav  2.928636\n",
            "neutral_sent001_short.wav  2.255678\n",
            " neutral_sent001_long.wav  2.441009\n",
            " neutral_sent003_long.wav  2.478161\n",
            "Device: cpu\n",
            "Model architecture: NISQA\n",
            "Loaded pretrained model from weights/nisqa_tts.tar\n",
            "# files: 10\n",
            "---> Predicting ...\n",
            "                      deg  mos_pred\n",
            " neutral_sent002_long.wav  2.237563\n",
            " neutral_sent003_long.wav  2.135721\n",
            "neutral_sent003_short.wav  2.748559\n",
            "neutral_sent004_short.wav  2.743794\n",
            "neutral_sent005_short.wav  3.521120\n",
            "neutral_sent002_short.wav  3.295687\n",
            " neutral_sent005_long.wav  2.353680\n",
            " neutral_sent004_long.wav  2.404449\n",
            "neutral_sent001_short.wav  2.307054\n",
            " neutral_sent001_long.wav  2.186512\n",
            "/content/drive/MyDrive/Colab_Notebooks/summerschool2023\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reference"
      ],
      "metadata": {
        "id": "AxhVW-dI4zC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[1] R. Skerry-Ryan et al., “Towards End-to-End Prosody Transfer for Expressive Speech Synthesis with Tacotron,” arXiv.org, 2018. https://arxiv.org/abs/1803.09047.\n",
        "\n",
        "[2] R. F. Kubichek, “Mel-cepstral distance measure for objective speech quality assessment,” Pacific Rim Conference on Communications, Computers and Signal Processing, May 1993, doi: https://doi.org/10.1109/pacrim.1993.407206.\n",
        "\n",
        "[3] gabrielmittag, “gabrielmittag/NISQA: NISQA - Non-Intrusive Speech Quality and TTS Naturalness Assessment,” GitHub, Mar. 22, 2022. https://github.com/gabrielmittag/NISQA.\n",
        "\n",
        "[4] Wei Chu and A. Alwan, “Reducing F0 Frame Error of F0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend,” 2009 IEEE International Conference on Acoustics, Speech and Signal Processing, Apr. 2009, doi: https://doi.org/10.1109/icassp.2009.4960497.\n",
        "\n",
        "[5] C.-C. Lo et al., “MOSNet: Deep Learning-Based Objective Assessment for Voice Conversion,” Interspeech 2019, Sep. 2019, doi: https://doi.org/10.21437/interspeech.2019-2003.\n",
        "\n",
        "[6] schmiph2, “schmiph2/pysepm: Python implementation of performance metrics in Loizou’s Speech Enhancement book,” GitHub, Jul. 14, 2020. https://github.com/schmiph2/pysepm .\n",
        "\n",
        "[7] H. Kawahara, H. Katayose, A. Cheveigné, and R. Patterson, “Fixed point analysis of frequency to instantaneous frequency mapping for accurate estimation of F0 and periodicity,” 6th European Conference on Speech Communication and Technology (Eurospeech 1999).\n",
        "\n",
        "[8] J. Ma, Y. Hu, and P. C. Loizou, “Objective measures for predicting speech intelligibility in noisy conditions based on new band-importance functions,” The Journal of the Acoustical Society of America, vol. 125, no. 5, p. 3387, 2009, doi: https://doi.org/10.1121/1.3097493.\n",
        "\n",
        "[9] “scipy.stats.pearsonr — SciPy v1.10.1 Manual,” Scipy.org, 2023. https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.pearsonr.html.\n",
        "\n",
        "[10] google, “google/visqol: Perceptual Quality Estimator for speech and audio,” GitHub, Jan. 03, 2023. https://github.com/google/visqol (accessed May 10, 2023).\n",
        "‌\n",
        "‌\n",
        "‌\n",
        "‌\n",
        "‌\n",
        "‌\n",
        "‌\n",
        "‌\n",
        "‌\n",
        "\n",
        "‌"
      ],
      "metadata": {
        "id": "cCRNXW5O5UzH"
      }
    }
  ]
}